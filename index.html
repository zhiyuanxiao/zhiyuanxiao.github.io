<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Zhiyuan Xiao</title>
  <meta name="Zhiyuan Xiao's Homepage" http-equiv="Content-Type" content="Zhiyuan Xiao's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Zhiyuan Xiao</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/ZhiyuanXiao.jpg"><img src="images/ZhiyuanXiao.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/ZhiyuanXiao_CV.pdf">CV</a> |
    <a href="mailto:zy.xiao.work@outlook.com">Email</a> |
    <!-- <a href="https://scholar.google.com">G Scholar</a> |
    <br/>
    | --> <a href="https://github.com/zhiyuanxiao/zhiyuanxiao.github.io">Github</a> | 
    <!-- <a href="https://twitter.com/zhiyuanxiao">Twitter</a> | 
    <a href="https://www.linkedin.com/in/zhiyuanxiao">LinkedIn</a> | -->
    </p>
    </td>
    <td width="70%" valign="top" align="justify">
    <p>
      I am a Research Assistant at School of Aeronautics and Astronautics, <a href="https://www.sysu.edu.cn/">Sun Yat-Sen University</a>, advised by Assoc Prof. <a href="">Qingrui Zhang</a>.
    </p>
    <p>
      I completed my Bachelor's in Mathematics and Applied Mathematics at <a href="https://www.scut.edu.cn/">South China University of Technology</a>.
    </p>
    <p>
      I enjoy playing table tennis and painting in my spare time. I designed the class uniform at SCUT.
    </p>
    <!-- <p>
      My research interests lie in the intersection of Robotics, Machine Learning and Reinforcement Learning. 
    </p> -->
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <!-- SYNLOCO -->
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://synloco.github.io">
    <video playsinline autoplay loop muted src="images/synloco-clip.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://synloco.github.io" id="SYNLOCO">
      <heading>SYNLOCO: Synthesizing Central Pattern Generator and Reinforcement Learning for Quadruped Locomotion</heading></a><br>
      Xinyu Zhang*, Zhiyuan Xiao*, Qingrui Zhang, Wei Pan<br>
      Accepted by CDC 2024<br>
      </p>

      <div class="paper" id="synloco">
      <a href="https://synloco.github.io">webpage</a> |
      <a href="https://synloco.github.io/resources/SYNLOCO.pdf">pdf</a> |
      <a href="javascript:toggleblock('synloco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('synloco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2310.06606">arXiv</a> |
      <!-- <a href="">code</a> | -->
      <a href="https://youtu.be/MXDZlLdwS4k">video</a>

      <p align="justify"> <i id="synloco_abs">The Central Pattern Generator (CPG) is adept at generating rhythmic gait patterns characterized by consistent timing and adequate foot clearance. Yet, its open-loop configuration often compromises the system's control performance in response to environmental variations. On the other hand, Reinforcement Learning (RL), celebrated for its model-free properties, has gained significant traction in robotics due to its inherent adaptability and robustness. However, initiating traditional RL approaches from the ground up presents computational challenges and a heightened risk of converging to suboptimal local minima. In this paper, we propose an innovative quadruped locomotion framework, SYNLOCO, by synthesizing CPG and RL that can ingeniously integrate the strengths of both methods, enabling the development of a locomotion controller that is both stable and natural. Furthermore, we introduce a set of performance-driven reward metrics that augment the learning of locomotion control. To optimize the learning trajectory of SYNLOCO, a two-phased training strategy is presented. Our empirical evaluation, conducted on a Unitree GO1 robot under varied conditions -- including distinct velocities, terrains, and payload capacities -- showcases SYNLOCO's ability to produce consistent and clear-footed gaits across diverse scenarios. The developed controller exhibits resilience against substantial parameter variations, underscoring its potential for robust real-world applications.</i></p>

<pre xml:space="preserve">
  @misc{zhang_synloco_2023,
        title = {{SYNLOCO}: Synthesizing Central Pattern
                  Generator and Reinforcement 
                  Learning for Quadruped Locomotion},
        url = {http://arxiv.org/abs/2310.06606},
        shorttitle = {{SYNLOCO}},
        publisher = {{arXiv}},
        author = {Zhang, Xinyu and Xiao, Zhiyuan and
                  Zhang, Qingrui and Pan, Wei},
        urldate = {2023-10-25},
        date = {2023-10-10},
        eprinttype = {arxiv},
        eprint = {2310.06606 [cs]},
        keywords = {Computer Science - Robotics}
  }
</pre>
      </div>
    </td>
  </tr>

  <!-- PALOCO -->
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://paloco-iros24.github.io">
    <video playsinline autoplay loop muted src="images/paloco-clip.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://paloco-iros24.github.io" id="PALOCO">
      <heading>PA-LOCO: Learning Perturbation-Adaptive Locomotion for Quadruped Robots</heading></a><br>
      Zhiyuan Xiao, Xinyu Zhang, Xiang Zhou, Qingrui Zhang<br>
      Accepted by IROS 2024<br>
      </p>

      <div class="paper" id="paloco">
      <a href="https://paloco-iros24.github.io">webpage</a> |
      <a href="https://paloco-iros24.github.io/resources/PALOCO.pdf">pdf</a> |
      <a href="javascript:toggleblock('paloco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('paloco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2407.04224">arXiv</a> |
      <!-- <a href="">code</a> | -->
      <a href="https://youtu.be/L6wWHvz3Zak">video</a>

      <p align="justify"> <i id="paloco_abs">Numerous locomotion controllers have been designed based on Reinforcement Learning (RL) to facilitate blind quadrupedal locomotion traversing challenging terrains.  Nevertheless, locomotion control is still a challenging task for quadruped robots traversing diverse terrains amidst unforeseen disturbances. Recently, privileged learning has been employed to learn reliable and robust quadrupedal locomotion over various terrains based on a teacher-student architecture. However, its one-encoder structure is not adequate in addressing external force perturbations. The student policy would experience inevitable performance degradation due to the feature embedding discrepancy between the feature encoder of the teacher policy and the one of the student policy. Hence, this paper presents a privileged learning framework with multiple feature encoders and a residual policy network for robust and reliable quadruped locomotion subject to various external perturbations. The multi-encoder structure can decouple latent features from different privileged information, ultimately leading to enhanced performance of the learned policy in terms of robustness, stability, and reliability. The efficiency of the proposed feature encoding module is analyzed in depth using extensive simulation data. The introduction of the residual policy network helps mitigate the performance degradation experienced by the student policy that attempts to clone the behaviors of a teacher policy. The proposed framework is evaluated on a Unitree GO1 robot, showcasing its performance enhancement over the state-of-the-art privileged learning algorithm through extensive experiments conducted on diverse terrains. Ablation studies are conducted to illustrate the efficiency of the residual policy network.</i></p>

<pre xml:space="preserve">
</pre>
      </div>
    </td>
  </tr>

  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="https://manipulation-locomotion.github.io">
    <video playsinline autoplay loop muted src="images/wbc-clip.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://manipulation-locomotion.github.io" id="MANIPLOCO">
      <heading>Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion</heading></a><br>
      Zipeng Fu*, Xuxin Cheng*, Deepak Pathak<br>
      CoRL 2022 (Oral)<br>
      <b style="color:rgb(255, 100, 100);">Best Systems Paper Award Finalist (top 0.79%)</b>
      </p>

      <div class="paper" id="maniploco">
      <a href="https://manipulation-locomotion.github.io">webpage</a> |
      <a href="https://arxiv.org/pdf/2210.10044.pdf">pdf</a> |
      <a href="javascript:toggleblock('maniploco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('maniploco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2210.10044">arXiv</a> |
      <a href="https://openreview.net/forum?id=zldI4UpuG7v">OpenReview</a> |
      <a href="https://www.youtube.com/watch?v=i9EdPl8uJUA">video</a>

      <p align="justify"> <i id="maniploco_abs">An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective and requires immense engineering to support coordination between the arm and legs, error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible where there is evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups.</i></p>

<pre xml:space="preserve">
@inproceedings{fu2022deep,
  author = {Fu, Zipeng and Cheng, Xuxin and 
            Pathak, Deepak},
  title = {Deep Whole-Body Control: Learning a Unified Policy
           for Manipulation and Locomotion},
  booktitle = {Conference on Robot Learning ({CoRL})},
  year = {2022}
}
</pre>
      </div>
    </td>
  </tr> -->

  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="https://energy-locomotion.github.io/">
    <video playsinline autoplay loop muted src="images/gait-clip.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://energy-locomotion.github.io/" id="ENERGYLOCO">
      <heading>Minimizing Energy Consumption Leads to the<br>Emergence of Gaits in Legged Robots</heading></a><br>
      Zipeng Fu, Ashish Kumar, Jitendra Malik, Deepak Pathak<br>
      CoRL 2021
      </p>

      <div class="paper" id="energyloco">
      <a href="https://energy-locomotion.github.io/">webpage</a> |
      <a href="https://arxiv.org/pdf/2111.01674.pdf">pdf</a> |
      <a href="javascript:toggleblock('energyloco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('energyloco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2111.01674">arXiv</a> |
      <a href="https://openreview.net/forum?id=PfC1Jr6gvuP">OpenReview</a> |
      <a href="https://www.youtube.com/watch?v=OQN5W2IAb9k">video</a>

      <p align="justify"> <i id="energyloco_abs">Legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. However, fixing a set of pre-programmed gaits limits the generality of locomotion. Recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. What principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? In this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. We demonstrate that learning to minimize energy consumption is sufficient for the emergence of natural locomotion gaits at different speeds in real quadruped robots. The emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. The same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. We validate our hypothesis in both simulation and real hardware across natural terrains.</i></p>

<pre xml:space="preserve">
@inproceedings{fu2021minimizing,
  author = {Fu, Zipeng and Kumar, Ashish and Malik, Jitendra and Pathak, Deepak},
  title = {Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots},
  booktitle = {Conference on Robot Learning (CoRL)},
  year = {2021}
}
</pre>
      </div>
    </td>
  </tr> -->


  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="https://ashish-kmr.github.io/rma-legged-robots/">
      <video playsinline autoplay loop muted src="images/rma-clip.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
      
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://ashish-kmr.github.io/rma-legged-robots/" id="RMA">
      <heading>RMA: Rapid Motor Adaptation for Legged Robots</heading></a><br>
      Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik<br>
      RSS 2021
      </p>

      <div class="paper" id="rma">
      <a href="https://ashish-kmr.github.io/rma-legged-robots/">webpage</a> |
      <a href="https://arxiv.org/pdf/2107.04034.pdf">pdf</a> |
      <a href="javascript:toggleblock('rma_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('rma')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2107.04034">arXiv</a> |
      <a href="https://youtu.be/nBy1piJrq1A">video</a>

      <p align="justify"> <i id="rma_abs">Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments.</i></p>

<pre xml:space="preserve">
@inproceedings{kumar2021rma,
  author = {Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
  title  = {RMA: Rapid Motor Adaptation for Legged Robots},
  booktitle = {Robotics: Science and Systems (RSS)},
  year   = {2021}
}
</pre>
      </div>
      <p>
        Media Coverage: <a href="https://ai.facebook.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions"> Facebook AI</a> |
        <a href=https://techcrunch.com/2021/07/09/stumble-proof-robot-adapts-to-challenging-terrain-in-real-time/?tpcc=ECTW2020">TechCrunch</a> |
        <a href="https://www.forbes.com/sites/martineparis/2021/07/09/facebook-robots-are-getting-smarter-watch-them-in-the-wild-video/?sh=3997033644ef">Forbes</a> |
        <a href="https://www.washingtonpost.com/technology/2021/07/12/facebook-ai-robotics/">Washington Post</a> |
        <a href="https://www.cnet.com/news/facebook-teaches-ai-powered-robot-to-adapt-while-walking/">CNet</a> |
        <a href="https://www.jiqizhixin.com/articles/2021-07-11">Synced Review</a> |
        <a href="https://engineering.berkeley.edu/news/2021/07/rapid-motor-adaptation-enables-robots-to-navigate-real-world/">UC Berkeley</a> |
        <a href="https://www.cmu.edu/news/stories/archives/2021/july/legged-robots-adapt.html">CMU</a>
      </p>
    </td>
  </tr> -->


  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/pdf/2110.00121.pdf"><img src="images/NeurIPS2019_CG_img1.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2110.00121.pdf" id="COLLAB_MARL">
      <heading>Emergence of Theory of Mind Collaboration in Multiagent Systems</heading></a><br>
      Luyao Yuan, Zipeng Fu, Linqi Zhou, Kexin Yang, Song-Chun Zhu<br>
      Emergent Communication Workshop<br>
      NeurIPS 2019</b>
      </p>

      <div class="paper" id="collab_marl">
      <a href="https://arxiv.org/pdf/2110.00121.pdf">pdf</a> |
      <a href="javascript:toggleblock('collab_marl_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('collab_marl')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2110.00121">arXiv</a> |
      <a href="https://github.com/MarkFzp/ToM-Collaboration">code</a>

      <p align="justify"> <i id="collab_marl_abs">Currently, in the study of multiagent systems, the intentions of agents are usually ignored. Nonetheless, as pointed out by Theory of Mind (ToM), people regularly reason about otherâ€™s mental states, including beliefs, goals, and intentions, to obtain performance advantage in competition, cooperation or coalition. However, due to its intrinsic recursion and intractable modeling of distribution over belief, integrating ToM in multiagent planning and decision making is still a challenge. In this paper, we incorporate ToM in multiagent partially observable Markov decision process (POMDP) and propose an adaptive training algorithm to develop effective collaboration between agents with ToM. We evaluate our algorithms with two games, where our algorithm surpasses all previous decentralized execution algorithms without modeling ToM.</i></p>

<pre xml:space="preserve">
@article{yuan2019emergencecollaboration,
  author = {Yuan, Luyao and Fu, Zipeng and Zhou, Linqi and Yang, Kexin and Zhu, Song-Chun},
  journal= {Emergent Communication Workshop at NeurIPS},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr> -->


  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="data/ACM_TURC2019_Adversarial.pdf"><img src="images/ACM_TURC2019_Adversarial_img.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/ACM_TURC2019_Adversarial.pdf" id="ACM_TURC">
      <heading>Adversarial Attack Against Scene Recognition System for Unmanned Vehicles</heading></a><br>
      Xuankai Wang, Mi Wen, Jinguo Li, Zipeng Fu and Rongxing Lu<br>
      ACM TURC 2019</b>
      </p>

      <div class="paper" id="acm_turc">
      <a href="data/ACM_TURC2019_Adversarial.pdf">pdf</a> |
      <a href="javascript:toggleblock('acm_turc_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('acm_turc')" class="togglebib">bibtex</a>

      <p align="justify"><i id="acm_turc_abs">Unmanned scene recognition means that unmanned vehicles can collect environmental data from equipped sensors and make decisions through algorithms, in which deep learning has become one of key technologies. Especially, with the discovery of adversarial examples against deep learning, the research on offensive and defensive against adversarial examples illustrates that the deep learning model for unmanned scene recognition also has the safety vulnerability. However, as far as we know, few studies have tried to explore the adversarial example attack in this field. Therefore, we try to address this problem by generating adversarial examples againist scene recognition classification model through experiments. In addition, we also try to improve the adversarial model robustness by the adversarial training. Extensive experiments have been conducted, and experimental results show that adversarial examples have an efficient attack effect on the neural network for scene recognition.</i></p>

<pre xml:space="preserve">
@article{wang2019adversarial,
  author = {Wang, Xuankai and Li, Jinguo and Fu, Zipeng and Lu, Rongxing},
  journal= {ACM TURC},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr> -->
</table>

<hr/>

<!-- Projects -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;Projects&nbsp;</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <!-- Perceptive Locomotion -->
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://perceptive-loco.github.io/">
    <video playsinline autoplay loop muted src="images/paloco-clip.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://perceptive-loco.github.io/" id="PerceptiveLOCO">
      <heading>Learning Robust Perceptive Locomotion for Quadrupedal Robots</heading></a><br>
      Zhiyuan Xiao<br>
      </p>

      <div class="paper" id="perceptive_loco">
      <a href="https://perceptive-loco.github.io/">webpage</a> |
      <a href="https://www.science.org/doi/10.1126/scirobotics.abk2822">reference</a> |
      <a href="javascript:toggleblock('perceptive_loco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('perceptive_loco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2407.04224">arXiv</a> |
      <!-- <a href="">code</a> | -->
      <a href="https://youtu.be/L6wWHvz3Zak">video</a>

      <p align="justify"> <i id="perceptive_loco_abs">Legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into underexplored areas. Exteroceptive perception is crucial for fast and energy-efficient locomotion: Perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability. However, using exteroceptive perception robustly for locomotion has remained a grand challenge in robotics. Snow, vegetation, and water visually appear as obstacles on which the robot cannot step or are missing altogether due to high reflectance. In addition, depth perception can degrade due to difficult lighting, dust, fog, reflective or transparent surfaces, sensor occlusion, and more. For this reason, the most robust and general solutions to legged locomotion to date rely solely on proprioception. This severely limits locomotion speed because the robot has to physically feel out the terrain before adapting its gait accordingly. Here, we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion. We leverage an attention-based recurrent encoder that integrates proprioceptive and exteroceptive input. The encoder is trained end to end and learns to seamlessly combine the different perception modalities without resorting to heuristics. The result is a legged locomotion controller with high robustness and speed. The controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour-long hike in the Alps in the time recommended for human hikers.</i></p>

<pre xml:space="preserve">
</pre>
      </div>
    </td>
  </tr>

</table>

<!-- Bibliography library -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;Bibliography Library&nbsp;</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <!-- SYNLOCO -->
  <tr>
    <td width="40%" valign="top" align="center"><a href="">
    <image src="images/Quadruped_ANYmal.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></image>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="data/Bibliography_Library.pdf" id="QUAD">
      <heading>Bibliography Library for Quadruped Locomotion</heading></a><br>
      Zhiyuan Xiao, Xinyu Zhang<br>
      Last update: 2024-03-15<br>
      </p>

      <div class="paper" id="quad">
      <a href="data/Bibliography_Library.pdf">pdf</a> |
      <a href="javascript:toggleblock('quadruped_abs')">abstract</a>

      <p align="justify"> <i id="quadruped_abs">Quadruped bibliography library</i></p>

      </div>
    </td>
  </tr>

</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Nostalgia</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td width="40%" valign="top" align="center"><a href="./images/trio_origin.JPG">
    <image  src="images/trio.jpg" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></iamge>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="./images/trio_origin.JPG" id="TRIO">
      <heading>Trio</heading></a><br>
      Jan. 2023. Shot in Nanao Island<br>
      </p>

      <div class="photo" id="tiro">
      <a href="./images/trio_origin.JPG">origin</a> |
      <a href="javascript:toggleblock('trio_abs')">fold</a>

      <p align="justify"> <i id="trio_abs">Waste of time. Nothing folded here.</i></p>
      
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="./images/mask.jpg">
    <image  src="images/mask.jpg" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></iamge>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="./images/mask.jpg" id="MASK">
      <heading>Mask</heading></a><br>
      Oct. 2022. Shot at Healthy Code Bridge<br>
      </p>

      <div class="photo" id="mask">
      <a href="./images/mask.jpg">origin</a> |
      <a href="javascript:toggleblock('mask_abs')">fold</a>

      <p align="justify"> <i id="mask_abs">Photographor: Zhamao the Great.</i></p>
      
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="./images/class_uniform.jpg">
    <image  src="images/class_uniform.jpg" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></iamge>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="./images/class_uniform.jpg" id="MASK">
      <heading>Design of Class Uniform</heading></a><br>
      Oct. 2020. <br>
      </p>

      <div class="photo" id="class_uniform">
      <a href="./images/class_uniform.jpg">origin</a> |
      <a href="javascript:toggleblock('class_uniform_abs')">fold</a>

      <p align="justify"> <i id="class_uniform_abs">Don't click the 'fold' button again.</i></p>
      
      </div>
    </td>
  </tr>

</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="http://www.cs.cmu.edu/~dpathak/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<!--<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script> -->

<!-- publications -->
<script xml:space="preserve" language="JavaScript">
  hideblock('synloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('paloco_abs');
</script>

<!-- projects -->
<script xml:space="preserve" language="JavaScript">
  hideblock('perceptive_loco_abs');
</script>


<!-- bibliography -->
<script xml:space="preserve" language="JavaScript">
  hideblock('quadruped_abs');
</script>

<!-- nostagia -->
<script xml:space="preserve" language="JavaScript">
  hideblock('trio_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mask_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('class_uniform_abs');
</script>
</body>

</html>
