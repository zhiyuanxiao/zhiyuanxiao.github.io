<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/fav_icon.jpg">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Zipeng Fu</title>
  <meta name="Zipeng Fu's Homepage" http-equiv="Content-Type" content="Zipeng Fu's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Zipeng Fu</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/ZipengFu.jpg"><img src="images/ZipengFu.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/ZipengFu_CV.pdf">CV</a> |
    <a href="mailto:zipengfu@cs.stanford.edu">Email</a> |
    <a href="https://scholar.google.com/citations?user=wMcPTbEAAAAJ&sortby=pubdate">G Scholar</a> |
    <br/>
    | <a href="https://github.com/MarkFzp">Github</a> | 
    <a href="https://twitter.com/zipengfu">Twitter</a> | 
    <a href="https://www.linkedin.com/in/zipengfu">LinkedIn</a> |
    </p>
    </td>
    <td width="70%" valign="top" align="justify">
    <p>
      I am an incoming PhD Student in Computer Science at <a href="https://ai.stanford.edu/">Stanford AI Lab</a>, supported by <a href="https://vpge.stanford.edu/fellowships-funding/sgf/">Stanford Graduate Fellowship</a>. 
    </p>
    <p>
      I was a Master's student in <a href="https://www.ml.cmu.edu/">Machince Learning Department</a> and a student researcher in <a href="https://www.ri.cmu.edu/">Robotics Institute</a> at <a href="https://www.cmu.edu/">CMU</a>, where I worked on robot learning, advised by <a href="https://www.cs.cmu.edu/~dpathak">Deepak Pathak</a>. I also collaborated closely with <a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> at UC Berkeley. 
    </p>
    <p>
      Previously, I completed my Bachelor's in Computer Science and Applied Math at <a href="https://www.ucla.edu/">UCLA</a>. I worked on multi-agent reinforcement learning and computational Theory of Mind with <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>.
    </p>
    <p>
      My research interests lie at the intersection of Machine Learning, Robotics and Computer Vision. 
    </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://navigation-locomotion.github.io/camera-ready/">
    <video playsinline autoplay loop muted src="images/nav-clip.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://navigation-locomotion.github.io/camera-ready/" id="NAVLOCO">
      <heading>Coupling Vision and Proprioception for</br>Navigation of Legged Robots</heading></a><br>
      Zipeng Fu*, Ashish Kumar*, Ananye Agarwal, Haozhi Qi,<br>Jitendra Malik, Deepak Pathak<br>
      CVPR 2022 (Best Paper at Multimodal Learning Workshop)
      </p>

      <div class="paper" id="navloco">
      <a href="https://navigation-locomotion.github.io/camera-ready/">webpage</a> |
      <a href="https://arxiv.org/pdf/2112.02094.pdf">pdf</a> |
      <a href="javascript:toggleblock('navloco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('navloco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2112.02094">arXiv</a> |
      <a href="https://github.com/MarkFzp/navigation-locomotion">code</a> |
      <a href="https://www.youtube.com/watch?v=sZVvutQUAQ4">video</a>

      <p align="justify"> <i id="navloco_abs">We exploit the complementary strengths of vision and proprioception to achieve point goal navigation in a legged robot. Legged systems are capable of traversing more complex terrain than wheeled robots, but to fully exploit this capability, we need the high-level path planner in the navigation system to be aware of the walking capabilities of the low-level locomotion policy on varying terrains. We achieve this by using proprioceptive feedback to estimate the safe operating limits of the walking policy, and to sense unexpected obstacles and terrain properties like smoothness or softness of the ground that may be missed by vision. The navigation system uses onboard cameras to generate an occupancy map and a corresponding cost map to reach the goal. The FMM (Fast Marching Method) planner then generates a target path. The velocity command generator takes this as input to generate the desired velocity for the locomotion policy using as input additional constraints, from the safety advisor, of unexpected obstacles and terrain determined speed limits. We show superior performance compared to wheeled robot (LoCoBot) baselines, and other baselines which have disjoint high-level planning and low-level control. We also show the real-world deployment of our system on a quadruped robot with onboard sensors and compute.</i></p>

<pre xml:space="preserve">
@article{fu2021coupling,
  author = {Fu, Zipeng and Kumar, Ashish and 
            Agarwal, Ananye and Qi, Haozhi and 
            Malik, Jitendra and Pathak, Deepak},
  title  = {Coupling Vision and Proprioception 
            for Navigation of Legged Robots},
  eprint = {2112.02094},
  archivePrefix = {arXiv},
  year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://energy-locomotion.github.io/">
    <video playsinline autoplay loop muted src="images/gait-clip.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://energy-locomotion.github.io/" id="ENERGYLOCO">
      <heading>Minimizing Energy Consumption Leads to the<br>Emergence of Gaits in Legged Robots</heading></a><br>
      Zipeng Fu, Ashish Kumar, Jitendra Malik, Deepak Pathak<br>
      CoRL 2021
      </p>

      <div class="paper" id="energyloco">
      <a href="https://energy-locomotion.github.io/">webpage</a> |
      <a href="https://arxiv.org/pdf/2111.01674.pdf">pdf</a> |
      <a href="javascript:toggleblock('energyloco_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('energyloco')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2111.01674">arXiv</a> |
      <a href="https://openreview.net/forum?id=PfC1Jr6gvuP">OpenReview</a> |
      <a href="https://www.youtube.com/watch?v=OQN5W2IAb9k">video</a>

      <p align="justify"> <i id="energyloco_abs">Legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. However, fixing a set of pre-programmed gaits limits the generality of locomotion. Recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. What principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? In this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. We demonstrate that learning to minimize energy consumption is sufficient for the emergence of natural locomotion gaits at different speeds in real quadruped robots. The emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. The same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. We validate our hypothesis in both simulation and real hardware across natural terrains.</i></p>

<pre xml:space="preserve">
@inproceedings{fu2021minimizing,
  author = {Fu, Zipeng and Kumar, Ashish and Malik, Jitendra and Pathak, Deepak},
  title  = {Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots},
  journal= {Conference on Robot Learning (CoRL)},
  year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://ashish-kmr.github.io/rma-legged-robots/">
      <video playsinline autoplay loop muted src="images/rma-clip.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
      <!-- <img src="images/rma.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"> -->
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://ashish-kmr.github.io/rma-legged-robots/" id="RMA">
      <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
      <heading>RMA: Rapid Motor Adaptation for Legged Robots</heading></a><br>
      Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik<br>
      RSS 2021
      </p>

      <div class="paper" id="rma">
      <a href="https://ashish-kmr.github.io/rma-legged-robots/">webpage</a> |
      <a href="https://arxiv.org/pdf/2107.04034.pdf">pdf</a> |
      <a href="javascript:toggleblock('rma_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('rma')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2107.04034">arXiv</a> |
      <a href="https://youtu.be/nBy1piJrq1A">video</a>

      <p align="justify"> <i id="rma_abs">Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments.</i></p>

<pre xml:space="preserve">
@inproceedings{kumar2021rma,
  author = {Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
  title  = {RMA: Rapid Motor Adaptation for Legged Robots},
  journal= {Robotics: Science and Systems (RSS)},
  year   = {2021}
}
</pre>
      </div>
      <p>
        Media Coverage: <a href="https://ai.facebook.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions"> Facebook AI</a> |
        <a href=https://techcrunch.com/2021/07/09/stumble-proof-robot-adapts-to-challenging-terrain-in-real-time/?tpcc=ECTW2020">TechCrunch</a> |
        <a href="https://www.forbes.com/sites/martineparis/2021/07/09/facebook-robots-are-getting-smarter-watch-them-in-the-wild-video/?sh=3997033644ef">Forbes</a> |
        <a href="https://www.washingtonpost.com/technology/2021/07/12/facebook-ai-robotics/">Washington Post</a> |
        <a href="https://www.cnet.com/news/facebook-teaches-ai-powered-robot-to-adapt-while-walking/">CNet</a> |
        <a href="https://www.jiqizhixin.com/articles/2021-07-11">Synced Review</a> |
        <a href="https://engineering.berkeley.edu/news/2021/07/rapid-motor-adaptation-enables-robots-to-navigate-real-world/">UC Berkeley</a> |
        <a href="https://www.cmu.edu/news/stories/archives/2021/july/legged-robots-adapt.html">CMU</a>
      </p>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="data/AAMAS2020_Imitation.pdf"><img src="images/AAMAS2020_Imitation_img.png" alt="sym" width="90%" style="padding-top:10px;padding-bottom:10px;border-radius:15px"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/AAMAS2020_Imitation.pdf" id="MULTI_IMITATION">
      <heading>Multi-Modal Imitation Learning in<br>Partially Observable Environments</heading></a><br>
      Zipeng Fu, Minghuan Liu, Ming Zhou, Weinan Zhang<br>
      Preprint 2020
      </p>

      <div class="paper" id="multi_imitation">
      <a href="data/AAMAS2020_Imitation.pdf">pdf</a> |
      <a href="javascript:toggleblock('multi_imitation_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('multi_imitation')" class="togglebib">bibtex</a> | 
      <a href="https://github.com/MarkFzp/infogail-pomdp">code</a>

      <p align="justify"> <i id="multi_imitation_abs">We consider imitation learning for agents to learn good policies from expert demonstration without any reward signal. Typical methods focus on single-expert single-task imitation in a fully observable environment. In practice, however, agents mostly make decisions based on their local observations, and the ability to generalize across various experts’ behaviors and multiple tasks is crucial for practical imitation learning. In this paper, we propose to take advantage of InfoGAIL with RNN-based belief state representations for multi-modal imitation learning in partially observable environments. We confirm the effectiveness of multi-expert learning of our method in a 2-dimensional environment, in which expert trajectories consist of two human-distinguishable behaviors. Further experimental results in continuous-control locomotion tasks reveal that our method can also disentangle interpretable latent factors in unlabeled multi-task demonstrations.</i></p>

<pre xml:space="preserve">
@article{fu2020multi,
  author = {Fu, Zipeng and Liu, Minghuan and Zhou, Weinan and Zhang, Minghuan},
  title  = {Multi-Modal Imitation Learning in Partially Observable Environments},
  journal= {Preprint},
  year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="data/ICAART2020_Reducing.pdf"><img src="images/ICAART2020_Reducing_img.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/ICAART2020_Reducing.pdf" id="VALUE_MARL">
      <heading>Reducing Overestimation of Value Mixing in Cooperative Deep</br>Multi-Agent Reinforcement Learning</heading></a><br>
      Zipeng Fu, Qingqing Zhao, Weinan Zhang<br>
      ICAART 2020
      </p>

      <div class="paper" id="value_marl">
      <a href="data/ICAART2020_Reducing.pdf">pdf</a> |
      <a href="javascript:toggleblock('value_marl_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('value_marl')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="value_marl_abs">Since the debut of Deep Q-Network (DQN), numerous researches have been conducted to integrate Deep Neural Networks (DNN) with Reinforcement Learning (RL). The tremendous expressive power of DNNs empowers Reinforcement Learning, which was mostly only functional in simple discrete / tabular settings, to solve complex problems in continuous and high-dimensional settings. Recently, Deep RL is adapted to Multi-Agent Systems (MAS). In many real-world scenarios, a group of agents, each with generally different local observations, needs to cooperate to achieve a collective reward. Despite decentralized execution, global state information can be shared among agents in a laboratory setting during the rehearsal period. We propose double QMIX, an end-to-end multi-agent Q-learning method with reduction of value overestimation, that trains decentralized agents’ policies in a centralized setting. The centralized Q-value is computed from each agent’s utility in a non-linear and anti-overestimated fashion. We provide the theoretical analysis of the reason why traditional DQN training methods lead to significant value overestimation in multi-agent setting, and how double QMIX solves this problem is explained. We also evaluate double QMIX in StarCraft II micromanagement environment to show a better performance, compared with other state-of-the-art value-based multi-agent reinforcement learning methods.</i></p>

<pre xml:space="preserve">
@article{fu2020reducing,
  author = {Fu, Zipeng and Zhao, Qingqing and Zhang, Weinan},
  title  = {Reducing Overestimation of Value Mixing in Cooperative Deep Multi-Agent Reinforcement Learning},
  journal= {ICAART},
  year   = {2020}
}
</pre>
      </div>
    </td>
  </tr>



  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/pdf/2110.00121.pdf"><img src="images/NeurIPS2019_CG_img1.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2110.00121.pdf" id="COLLAB_MARL">
      <heading>Emergence of Theory of Mind Collaboration in Multiagent Systems</heading></a><br>
      Luyao Yuan, Zipeng Fu, Linqi Zhou, Kexin Yang, Song-Chun Zhu<br>
      Emergent Communication Workshop<br>
      NeurIPS 2019</b>
      </p>

      <div class="paper" id="collab_marl">
      <a href="https://arxiv.org/pdf/2110.00121.pdf">pdf</a> |
      <a href="javascript:toggleblock('collab_marl_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('collab_marl')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2110.00121">arXiv</a> |
      <a href="https://github.com/MarkFzp/ToM-Collaboration">code</a>

      <p align="justify"> <i id="collab_marl_abs">Currently, in the study of multiagent systems, the intentions of agents are usually ignored. Nonetheless, as pointed out by Theory of Mind (ToM), people regularly reason about other’s mental states, including beliefs, goals, and intentions, to obtain performance advantage in competition, cooperation or coalition. However, due to its intrinsic recursion and intractable modeling of distribution over belief, integrating ToM in multiagent planning and decision making is still a challenge. In this paper, we incorporate ToM in multiagent partially observable Markov decision process (POMDP) and propose an adaptive training algorithm to develop effective collaboration between agents with ToM. We evaluate our algorithms with two games, where our algorithm surpasses all previous decentralized execution algorithms without modeling ToM.</i></p>

<pre xml:space="preserve">
@article{yuan2019emergencecollaboration,
  author = {Yuan, Luyao and Fu, Zipeng and Zhou, Linqi and Yang, Kexin and Zhu, Song-Chun},
  journal= {Emergent Communication Workshop at NeurIPS},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/pdf/2001.07752.pdf"><img src="images/AAMAS2020_Pragmatics_img.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2001.07752.pdf" id="PRAGMATICS_MARL">
      <heading>Emergence of Pragmatics from Referential Game between<br>Theory of Mind Agents</heading></a><br>
      Luyao Yuan, Zipeng Fu, Jingyue Shen, Lu Xu, Junhong Shen,</br>Song-Chun Zhu<br>
      Emergent Communication Workshop<br>
      NeurIPS 2019</b>
      </p>

      <div class="paper" id="pragmatics_marl">
      <a href="https://arxiv.org/pdf/2001.07752.pdf">pdf</a> |
      <a href="javascript:toggleblock('pragmatics_marl_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('pragmatics_marl')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2001.07752">arXiv</a> |
      <a href="https://github.com/MarkFzp/ToM-Pragmatics">code</a>

      <p align="justify"> <i id="pragmatics_marl_abs">Pragmatics studies how context can contribute to language meanings. In human communication, language is never interpreted out of context, and sentences can usually convey more information than their literal meanings. However, this mechanism is missing in most multi-agent systems, restricting the communication efficiency and the capability of human-agent interaction. In this paper, we propose an algorithm, using which agents can spontaneously learn the ability to “read between lines” without any explicit hand-designed rules. We integrate theory of mind (ToM) in a cooperative multi-agent pedagogical situation and propose an adaptive reinforcement learning (RL) algorithm to develop a communication protocol. ToM is a profound cognitive science concept, claiming that people regularly reason about other’s mental states, including beliefs, goals, and intentions, to obtain performance advantage in competition, cooperation or coalition. With this ability, agents consider language as not only messages but also rational acts reflecting others hidden states. Our experiments demonstrate the advantage of pragmatic protocols over non-pragmatic protocols. We also show the teaching complexity following the pragmatic protocol empirically approximates to recursive teaching dimension (RTD).</i></p>

<pre xml:space="preserve">
@article{yuan2019emergencepragmatics,
  author = {Yuan, Luyao and Fu, Zipeng and Shen, Jingyue and Xu, Lu and Shen, Junhong and Zhu, Song-Chun},
  journal= {Emergent Communication Workshop at NeurIPS},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="data/AOG_MCTS.pdf"><img src="images/aog_mcts_img2.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/AOG_MCTS.pdf" id="AOG_MCTS">
      <heading>Unsupervised Incremental Structure Learning of Stochastic And-Or Grammars with Monte Carlo Tree Search</heading></a><br>
      Luyao Yuan, Jingyue Shen, Zipeng Fu, Song-Chun Zhu<br>
      Preprint 2019</b>
      </p>

      <div class="paper" id="aog_mcts">
      <a href="data/AOG_MCTS.pdf">pdf</a> |
      <a href="javascript:toggleblock('aog_mcts_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('aog_mcts')" class="togglebib">bibtex</a> | 
      <a href="https://github.com/MarkFzp/and-or-graph-lib">code (And-Or-Graph Learning Library)</a>

      <p align="justify"><i id="aog_mcts_abs">Stochastic And-Or grammars form a compact representation of probabilistic contextfree grammars. They explicitly model compositionality and reconfigurability in a hierarchical manner and can be utilized to understand the underlying structures of different kinds of data (e.g., language, image, or video). In this paper, we proposed an unsupervised AndOr grammar learning approach that iteratively searches for better grammar structure and parameters to optimize the grammar compactness and data likelihood. To handle the complexity of grammar learning, we developed an algorithm based on the Monte Carlo Tree Search to effectively explore the search space. Also, our method enables incremental grammar learning. Experimental results show that our approach significantly outperforms previous greedy-search-based approaches, and our incremental learning results are comparable to previous batch learning results.</i></p>

<pre xml:space="preserve">
@article{yuan2019stochastic,
  author = {Yuan, Luyao and Shen, Jingyue and Fu, Zipeng and Zhu, Song-Chun},
  journal= {Preprint},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>


    
  <tr>
    <td width="40%" valign="top" align="center"><a href="data/JNCS2019_Review.pdf"><img src="images/JNSC2019_Review_img.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/JNCS2019_Review.pdf" id="MATERIAL_REVIEW">
      <heading>Machine Learning for Glass Science and Engineering: A Review</heading></a><br>
      Han Liu, Zipeng Fu, Kai Yang, Xinyi Xu, Mathieu Bauchy<br>
      Journal of Non-Crystalline Solids 2019</b>
      </p>

      <div class="paper" id="material_review">
      <a href="data/JNCS2019_Review.pdf">pdf</a> |
      <a href="javascript:toggleblock('material_review_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('material_review')" class="togglebib">bibtex</a>

      <p align="justify"><i id="material_review_abs">The design of new glasses is often plagued by poorly efficient Edisonian “trial-and-error” discovery approaches. As an alternative route, the Materials Genome Initiative has largely popularized new approaches relying on artificial intelligence and machine learning for accelerating the discovery and optimization of novel, advanced materials. Here, we review some recent progress in adopting machine learning to accelerate the design of new glasses with tailored properties.</i></p>

<pre xml:space="preserve">
@article{liu2019machine,
  author = {Liu, Han and Fu, Zipeng and Yang, Kai and Xu, Xinyi and Bauchy, Mathieu},
  journal= {Journal of Non-Crystalline Solids},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="data/ACM_TURC2019_Adversarial.pdf"><img src="images/ACM_TURC2019_Adversarial_img.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/ACM_TURC2019_Adversarial.pdf" id="ACM_TURC">
      <heading>Adversarial Attack Against Scene Recognition System for Unmanned Vehicles</heading></a><br>
      Xuankai Wang, Mi Wen, Jinguo Li, Zipeng Fu and Rongxing Lu<br>
      ACM TURC 2019</b>
      </p>

      <div class="paper" id="acm_turc">
      <a href="data/ACM_TURC2019_Adversarial.pdf">pdf</a> |
      <a href="javascript:toggleblock('acm_turc_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('acm_turc')" class="togglebib">bibtex</a>

      <p align="justify"><i id="acm_turc_abs">Unmanned scene recognition means that unmanned vehicles can collect environmental data from equipped sensors and make decisions through algorithms, in which deep learning has become one of key technologies. Especially, with the discovery of adversarial examples against deep learning, the research on offensive and defensive against adversarial examples illustrates that the deep learning model for unmanned scene recognition also has the safety vulnerability. However, as far as we know, few studies have tried to explore the adversarial example attack in this field. Therefore, we try to address this problem by generating adversarial examples againist scene recognition classification model through experiments. In addition, we also try to improve the adversarial model robustness by the adversarial training. Extensive experiments have been conducted, and experimental results show that adversarial examples have an efficient attack effect on the neural network for scene recognition.</i></p>

<pre xml:space="preserve">
@article{wang2019adversarial,
  author = {Wang, Xuankai and Li, Jinguo and Fu, Zipeng and Lu, Rongxing},
  journal= {ACM TURC},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  
  <tr>
    <td width="40%" valign="top" align="center"><a href="data/IEEE_IoT2019_Energy.pdf"><img src="images/IEEE_IoT2019_Energy_img.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></a></td>
    <td width="60%" valign="top">
      <p><a href="data/IEEE_IoT2019_Energy.pdf" id="IEEE_IoT">
      <heading>Energy Theft Detection With Energy Privacy Preservation in the Smart Grid</heading></a><br>
      Donghuan Yao, Mi Wen, Xiaohui Liang, Zipeng Fu, Kai Zhang, Baojia Yang<br>
      IEEE IoT Journal 2019</b>
      </p>

      <div class="paper" id="ieee_iot">
      <a href="data/IEEE_IoT2019_Energy.pdf">pdf</a> |
      <a href="javascript:toggleblock('ieee_iot_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('ieee_iot')" class="togglebib">bibtex</a>

      <p align="justify"><i id="ieee_iot_abs">As a prominent early instance of the Internet of Things in the smart grid, the advanced metering infrastructure (AMI) provides real-time information from smart meters to both grid operators and customers, exploiting the full potential of demand response. However, the newly collected information without security protection can be maliciously altered and result in huge loss. In this paper, we propose an energy theft detection scheme with energy privacy preservation in the smart grid. Especially, we use combined convolutional neural networks (CNNs) to detect abnormal behavior of the metering data from a long-period pattern observation. In addition, we employ Paillier algorithm to protect the energy privacy. In other words, the users’ energy data are securely protected in the transmission and the data disclosure is minimized. Our security analysis demonstrates that in our scheme data privacy and authentication are both achieved. Experimental results illustrate that our modified CNN model can effectively detect abnormal behaviors at an accuracy up to 92.67%.</i></p>

<pre xml:space="preserve">
@article{yao2019energy,
  author = {Yao, Donghuan and Liang, Xiaohui and Fu, Zipeng and Zhang, Kai and Yang, Baojia},
  journal= {IEEE Internet of Things Journal},
  year   = {2019}
}
</pre>
      </div>
    </td>
  </tr>

</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="http://www.cs.cmu.edu/~dpathak/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('value_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('multi_imitation_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('hndp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
</body>

</html>
